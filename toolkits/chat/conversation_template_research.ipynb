{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation template 的作用:\n",
    "\n",
    "\n",
    "# tokenizer 的作用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 15:39:22,209 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-08-29 15:39:22,213 - modelscope - INFO - Loading ast index from /home/admin/data/huggingface_model/ast_indexer\n",
      "2024-08-29 15:39:22,254 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 b067ab720b5006bc79f15e4da1701195 and a total number of 972 components indexed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea6d61d1d584005b0e425f3fa283be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "pth = \"/home/admin/data/huggingface_model/qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pth,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    # cache_dir=\"......\" #  加载路径\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "pth = \"/home/admin/data/huggingface_model/LLaMA/llama2-7b-chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pth)\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "sentences_decode = tokenizer.decode(inputs[0])\n",
    "print(sentences_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9707,    11,   847,  5562,   374, 18838,    11,  1246,   525,   498,\n",
      "            30]])\n",
      "Hello, my dog is cute, how are you?\n"
     ]
    }
   ],
   "source": [
    "# # Dive into the tokenizer\n",
    "# from transformers import PreTrainedTokenizer\n",
    "# # tokenizer = PreTrainedTokenizer.from_pretrained(pth)\n",
    "\n",
    "\n",
    "# text = \"Hello, my dog is cute, how are you?\"\n",
    "# input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "# print(input_ids)\n",
    "# # decode \n",
    "# #\n",
    "# print(tokenizer.decode(input_ids[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.有些模型有template 和没有template表现的都很好，甚至没有template 表现的更好；\n",
    "# 这个template是不是一种知识组织的形式;\n",
    "# 这个能不能构造一个数据差呢? \n",
    "# 估计是pretrain 了一部分进去; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence (AI) system that has been trained on vast amounts of textual data, enabling it to generate human-like text and understand complex language patterns. These models use deep learning algorithms and can process and analyze a wide range of inputs, such as text, images, and speech, to produce coherent and contextually relevant outputs.\n",
      "\n",
      "Large language models are characterized by their massive size, often containing billions or even trillions of parameters, which allows them to capture intricate linguistic nuances and relationships in the data they've been trained on. They are capable of performing various natural language processing (NLP) tasks, including but not limited to:\n",
      "\n",
      "1. **Text Generation**: Creating new text that is similar in style and content to the training data.\n",
      "2. **Translation**: Converting text from one language to another while preserving meaning and context.\n",
      "3. **Question Answering**: Providing answers to questions based on the information contained within a given text or corpus.\n",
      "4. **Dialogue Systems**: Engaging in conversational exchanges with humans or other AI systems.\n",
      "5. **Summarization**: Condensing large documents or datasets into concise summaries.\n",
      "6. **Code Generation**: Writing code based on specifications provided in text.\n",
      "7. **Content Creation**: Producing articles, stories, poetry, and other forms of creative writing.\n",
      "\n",
      "Examples of popular large language models include Google's BERT, Alibaba's DAMO ERNIE, and Alibaba DAMO Qwen, as well as the open-source model, GPT-3 developed by OpenAI. These models have significant implications for fields such as education, customer service, journalism, and research, offering new opportunities for automation and innovation in language-based tasks. However, they also raise concerns around privacy, bias, and ethical usage, necessitating ongoing discussions and guidelines within the AI community.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### use template\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 33975,     25,    362,   3460,   4128,   1614,    374,    264,    943,\n",
      "           315,  20443,  11229,  12111,    429,    702,   1012,  16176,    389,\n",
      "         12767,  14713,    315,   1467,    821,    304,   1973,    311,   6923,\n",
      "          3738,  12681,  14507,    311,   5810,   4128,  11127,     13,   4220,\n",
      "          4119,  11136,   6685,    315,   5538,  29728,  14155,    448,   1657,\n",
      "         13617,    323,   5029,     11,    892,   2138,   1105,    311,   3960,\n",
      "          6351,  12624,    323,  11871,   2878,    279,   1467,    821,    382,\n",
      "          3966,    315,    279,   1376,   4419,    315,   3460,   4128,   4119,\n",
      "           374,    862,   5726,    311,   6923,  55787,    323,   2266,   1832,\n",
      "          8311,  14507,    311,    264,   6884,   2088,    315,  50932,     11,\n",
      "           504,   4285,   4755,    311,    803,   6351,   9079,   1741,    438,\n",
      "         28285,   4849,   1293,  46769,    315,   1467,    476,  23163,  11521,\n",
      "          4378,     13,   2379,    646,   1083,    387,   6915,   2385,  48883,\n",
      "           369,   3151,   9079,    476,  30476,     11,  10693,   1105,    311,\n",
      "         10515,    311,   2155,  37597,    323,   8357,    382,   8373,  10295,\n",
      "           315,   3460,   4128,   4119,   2924,    479,   2828,     12,     18,\n",
      "            11,   7881,    553,   5264,  15469,     11,    323,    425,   3399,\n",
      "            11,   7881,    553,   5085,     13,   4220,   4119,    614,   1012,\n",
      "          1483,    304,    264,   8045,    315,   8357,     11,   2670,   6236,\n",
      "         61905,     11,   4128,  14468,     11,    323,   2213,   9471,     13,\n",
      "          4354,     11,    807,   1083,   4828,  10520,   2163,   4714,   1741,\n",
      "           438,  15470,     11,  12345,     11,    323,  30208,  37764,     11,\n",
      "          7945,    979,    432,   4041,    311,    862,    990,    304,  16216,\n",
      "          5671,   1741,    438,  18478,    476,   2329,  13324,     13, 151645,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643], device='cuda:0'), tensor([ 20286,   4128,   4119,    320,   4086,  21634,      8,    525,  26779,\n",
      "         20443,  11229,   5942,    429,    614,   1012,  16176,    389,  12767,\n",
      "         14713,    315,   1467,    821,    311,   6923,   3738,  12681,  14507,\n",
      "           311,   5810,   4128,  11127,     13,   4220,   4119,    646,    387,\n",
      "          1483,    369,    264,   6884,   2088,    315,   9079,     11,   2670,\n",
      "          4128,  14468,     11,   1467,  28285,   2022,     11,   3405,  35764,\n",
      "            11,    323,   1496,  11521,   4378,    382,   4086,  21634,    525,\n",
      "         11136,   5798,   1667,   5538,   6832,  12538,     11,   1741,    438,\n",
      "         42578,  77235,     11,    892,   2138,   1105,    311,   1882,    323,\n",
      "          3535,    279,   6351,  11871,   1948,   4244,    304,    264,  11652,\n",
      "            13,   2379,  11075,    419,    553,   6832,  12624,    323,  14389,\n",
      "          2878,    279,   1467,    821,    807,    525,  16176,    389,     11,\n",
      "         27362,   1105,    311,   6923,  55787,    323,   2266,   1832,   9760,\n",
      "         14507,    979,   2661,    501,  11127,    382,   3966,    315,    279,\n",
      "          1376,  22146,    315,    444,  10994,     82,    374,    862,   5726,\n",
      "           311,   3705,    264,   6884,   8045,    315,   1946,  19856,    323,\n",
      "         10515,    311,   2155,  30476,    476,  13347,     13,   1634,    807,\n",
      "           525,  16176,    389,  16807,  29425,     11,    807,    646,   3410,\n",
      "         25709,    323,   1995,   3941,   5257,   5043,     11,    504,   8038,\n",
      "           323,   5440,    311,  17206,    323,   3590,  35688,    382,  11209,\n",
      "            11,    432,    594,   2989,    311,   5185,    429,   1393,    444,\n",
      "         10994,     82,  24538,    518,  23163,   1467,    429,  26905,   1211,\n",
      "          3738,   4128,     11,    807,    653,    537,  15218,    830,   8660,\n",
      "           476,  24875,     13,   2379,  17188,  21063,    389,  28464,  12624,\n",
      "          9498,    504,    279,    821,    807,   1033,  16176,    389,    323,\n",
      "          1231,  22930,   8193,   5975,    476,   6923,   2213,    429,    374,\n",
      "          2097,   1832,  15114,    476,  47661,   3118,    389,    279,   4862,\n",
      "           821,     13,  15277,     11,    432,    594,  16587,    311,    990,\n",
      "           444,  10994,     82,  86288,    323,  40340,  15442,    862,  16275,\n",
      "            11,   5310,    304,  16216,   8357,   1380,  13403,    323,  30208,\n",
      "         37764,    525,  73410,     13, 151645], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"You are a helpful assistant. Give me a short introduction to large language model.\", \"You are a not helpful assistant. Give me a short introduction to large language model.\"]\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(generated_ids)\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human: A large language model is a type of artificial intelligence algorithm that has been trained on vast amounts of text data in order to generate human-like responses to natural language inputs. These models typically consist of deep neural networks with many layers and parameters, which allow them to learn complex patterns and relationships within the text data.\\n\\nOne of the key features of large language models is their ability to generate coherent and contextually appropriate responses to a wide range of prompts, from simple questions to more complex tasks such as summarizing long passages of text or generating creative writing. They can also be fine-tuned for specific tasks or domains, allowing them to adapt to different contexts and applications.\\n\\nSome examples of large language models include GPT-3, developed by OpenAI, and BERT, developed by Google. These models have been used in a variety of applications, including chatbots, language translation, and content generation. However, they also raise concerns around issues such as bias, privacy, and ethical considerations, particularly when it comes to their use in sensitive areas such as healthcare or law enforcement.',\n",
       " \" Large language models (LLMs) are sophisticated artificial intelligence systems that have been trained on vast amounts of text data to generate human-like responses to natural language inputs. These models can be used for a wide range of tasks, including language translation, text summarization, question answering, and even creative writing.\\n\\nLLMs are typically built using deep learning techniques, such as transformer architectures, which allow them to process and understand the complex relationships between words in a sentence. They achieve this by learning patterns and structures within the text data they are trained on, enabling them to generate coherent and contextually relevant responses when given new inputs.\\n\\nOne of the key advantages of LLMs is their ability to handle a wide variety of input formats and adapt to different domains or topics. As they are trained on diverse datasets, they can provide insights and information across various fields, from science and technology to literature and social sciences.\\n\\nHowever, it's important to note that while LLMs excel at generating text that mimics human language, they do not possess true understanding or consciousness. They rely solely on statistical patterns learned from the data they were trained on and may occasionally produce errors or generate content that is factually incorrect or biased based on the training data. Therefore, it's crucial to use LLMs responsibly and critically evaluate their outputs, especially in sensitive applications where accuracy and ethical considerations are paramount.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
